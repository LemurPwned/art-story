{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "j9Za_ErMg5t-",
    "outputId": "a13dc0db-0e78-4a4b-8667-ea0441133595"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lemurpwned/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QwC2tN0Mh-4M",
    "outputId": "766188a7-1dc2-45b3-f758-3bcaf66c5acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size 108824\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(source='long_text.txt'):\n",
    "  eng_stopwords = stopwords.words(\"english\")\n",
    "  words = re.findall(r'\\w+', open(source).read().lower())\n",
    "  filtered_words = filter(lambda x: x not in eng_stopwords, words)\n",
    "  word_dict = {w: i for i, w in enumerate(filtered_words)}\n",
    "  return word_dict\n",
    "\n",
    "word_dict = tokenize_text()\n",
    "print(f\"Dictionary size {len(word_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MNNWr328r7wv",
    "outputId": "c563df61-fef7-4772-9b37-0eee68c31972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences 154927\n"
     ]
    }
   ],
   "source": [
    "def clear_sentence(sentence):\n",
    "    for character in [\n",
    "            \"\\n\", \"\\\"\", \"\\'\", \"\\`\", \"!\", \":\", \";\", \".\", \",\", \"?\", '-', '[',\n",
    "            ']', \"\\\\\", \"|\", \"\\t\", \"#\", \"-\", '(', ')', '{', '}', \"‚Äù\"\n",
    "    ]:\n",
    "        sentence = sentence.replace(character, '')\n",
    "    return sentence\n",
    "\n",
    "new_sentences = []\n",
    "with open('./long_text.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if 'http' in line:\n",
    "            continue\n",
    "        sents = line.split('.')\n",
    "        for sent in sents:\n",
    "            sent = clear_sentence(sent.strip())\n",
    "            words = sent.split(' ')\n",
    "            filtered_words = list(filter(lambda x: (x != ' ') and (x != ''), words))\n",
    "            if len(filtered_words) < 6:\n",
    "                continue\n",
    "            new_sentences.append(' '.join(filtered_words).lower())\n",
    "\n",
    "# print(new_sentences[:150])\n",
    "print(f\"Sentences {len(new_sentences)}\")\n",
    "with open('./sentenced_long_text.txt', 'w') as f:\n",
    "    for item in new_sentences:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhHXbKvfiuC7"
   },
   "outputs": [],
   "source": [
    "import multiprocessing \n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence \n",
    "\n",
    "\n",
    "w2v = Word2Vec(min_count=15,\n",
    "               window=5,\n",
    "               size=300,\n",
    "               sample=6e-5,\n",
    "               min_alpha=0.0007, \n",
    "               negative=20,\n",
    "               workers=multiprocessing.cpu_count()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "uxoaDR-xkml0",
    "outputId": "29f804b1-03a3-4252-e228-d78d0debf004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.04 mins\n"
     ]
    }
   ],
   "source": [
    "sentences = LineSentence('./sentenced_long_text.txt')\n",
    "t = time.time()\n",
    "w2v.build_vocab(sentences, progress_per=10000)\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "eGyraF4qyiJ-",
    "outputId": "0d89ca18-5b11-4f96-845b-bf6fa5aa6852"
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "w2v.train(sentences, total_examples=w2v.corpus_count, epochs=30, \n",
    "                report_delay=1, compute_loss=True)\n",
    "print(f\"Time to train the model: {round((time.time() - t) / 60, 2)} mins\")\n",
    "print(f\"Loss {w2v.get_latest_training_loss()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oRG5gbdzdBF"
   },
   "outputs": [],
   "source": [
    "wv = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "a7oxgFXqAaRc",
    "outputId": "dd86fbc3-9ba0-4973-914d-12644ad2e079"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "genres_prop = ['renaissance', 'surrealism', 'constructivism', 'modernism', 'baroque',\n",
    "          'cubism', 'impressionism', 'minimalism', 'classicism', 'graffiti']\n",
    "genres = []\n",
    "genre_vectors = []\n",
    "for genre in genres_prop:\n",
    "  try:\n",
    "    genre_vectors.append(wv[genre])\n",
    "    genres.append(genre)\n",
    "  except KeyError:\n",
    "    print(f\"Genre {genre} is missing from the vocab.\")\n",
    "    len_gen -= 1\n",
    "\n",
    "genre_vectors = np.array(genre_vectors)\n",
    "\n",
    "\n",
    "painters_prop = ['picasso', 'monet', 'rembrandt', 'vinci', \n",
    "                 'frida', 'michelangelo', 'caravaggio', 'rubens',\n",
    "                  'banksy']\n",
    "painter_vectors = []\n",
    "painters = []\n",
    "for painter in painters_prop:\n",
    "  try:\n",
    "    painter_vectors.append(wv[painter])\n",
    "    painters.append(painter)\n",
    "  except KeyError:\n",
    "    print(f\"Painter {painter} is missing from the vocab.\")\n",
    "  \n",
    "painter_vectors = np.array(painter_vectors)\n",
    "vectors = np.concatenate([genre_vectors, painter_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "V2oZEIYNAfpH",
    "outputId": "bd7be0cf-7a2c-4cb3-b1f2-cb4798889e4b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "len_gen = len(genres)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=35)\n",
    "pca = PCA(n_components=2)\n",
    "Y = tsne.fit_transform(vectors)\n",
    "Y = pca.fit_transform(vectors)\n",
    "\n",
    "plt.scatter(Y[:len_gen, 0], Y[:len_gen, 1])\n",
    "plt.scatter(Y[len_gen:, 0], Y[len_gen:, 1])\n",
    "for i, txt in enumerate(genres):\n",
    "  plt.annotate(txt, (Y[i,0], Y[i, 1]))\n",
    "for i, txt in enumerate(painters):\n",
    "  plt.annotate(txt, (Y[i + len_gen,0], Y[i+len_gen, 1]))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "W1Zw65wTCi2-",
    "outputId": "fd3d44a9-fb57-41e5-bc79-3d83d171a3a5"
   },
   "outputs": [],
   "source": [
    "for painter in painters_prop:\n",
    "  mst_sml = 0 \n",
    "  mst_sml_lab = None\n",
    "  for genre in genres_prop:\n",
    "    try:\n",
    "      smlr = wv.similarity(painter, genre)\n",
    "    except KeyError: continue\n",
    "    if smlr > mst_sml:\n",
    "      mst_sml = smlr\n",
    "      mst_sml_lab = genre\n",
    "  if mst_sml_lab is not None:\n",
    "    print(f\"Painter {painter}, most_similar {mst_sml_lab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bF7mzwdmUCTL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ArtStoryTest.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
